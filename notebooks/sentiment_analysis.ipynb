{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28307b59dc547f5a092fc8e15eafcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" I hate u\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7468, -0.5561, -2.0293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 0.9567\n",
      "2) 0.0352\n",
      "3) 0.0081\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 0.9797317385673523\n",
      "hate: 0.9996899366378784\n",
      "you.: 0.9996235370635986\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Your sentence\n",
    "sentence = \"I hate you.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.split()\n",
    "\n",
    "# Get the sentiment scores for each word\n",
    "word_sentiments = {word: sentiment_analysis(word)[0]['score'] for word in words}\n",
    "\n",
    "# Print the sentiment scores for each word\n",
    "for word, sentiment_score in word_sentiments.items():\n",
    "    print(f'{word}: {sentiment_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': {'label': 'POSITIVE', 'score': 0.9797317385673523},\n",
       " 'hate': {'label': 'NEGATIVE', 'score': 0.9996899366378784},\n",
       " 'you.': {'label': 'POSITIVE', 'score': 0.9996235370635986}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word: sentiment_analysis(word)[0] for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='i hate u'\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Get the sentiment scores for each word\n",
    "word_sentiments = {word: sid.polarity_scores(word) for word in words}\n",
    "\n",
    "# Print the sentiment scores for each word\n",
    "# for word, sentiment_score in word_sentiments.items():\n",
    "#     print(f'{word}: {sentiment_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0},\n",
       " 'hate': {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5719},\n",
       " 'u': {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentence = \"I hate this product. It's amazing!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence into words\n",
    "words = sentence.split()\n",
    "\n",
    "# Get the sentiment scores for each word\n",
    "word_sentiments = [sid.polarity_scores(word)['compound'] for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, -0.5719, 0.0, 0.0, 0.0, 0.6239]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'P'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb 单元格 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI love this product. It\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms amazing!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m word_sentiments \u001b[39m=\u001b[39m get_word_sentiments(sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;00m word, sentiment \u001b[39min\u001b[39;00m word_sentiments:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00msentiment\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb 单元格 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         word_pos \u001b[39m=\u001b[39m pos_mapping[word_pos]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# Get the sentiment score for the word\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     sentiment \u001b[39m=\u001b[39m get_sentiment(word, word_pos)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     word_sentiments\u001b[39m.\u001b[39mappend((word, sentiment))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m word_sentiments\n",
      "\u001b[1;32m/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb 单元格 20\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sentiment\u001b[39m(word, pos):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Get the sentiment scores for a given word and part of speech\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     synsets \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(swn\u001b[39m.\u001b[39;49msenti_synsets(word, pos))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m synsets:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m# Use the average sentiment scores of all the synsets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504d4e4c503033227d/data/liukai/space/CEM/notebooks/sentiment_analysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         sentiment \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(s\u001b[39m.\u001b[39mpos_score() \u001b[39m-\u001b[39m s\u001b[39m.\u001b[39mneg_score() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m synsets) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(synsets)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/nlpenv/lib/python3.10/site-packages/nltk/corpus/reader/sentiwordnet.py:94\u001b[0m, in \u001b[0;36mSentiWordNetCorpusReader.senti_synsets\u001b[0;34m(self, string, pos)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m wordnet \u001b[39mas\u001b[39;00m wn\n\u001b[1;32m     93\u001b[0m sentis \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 94\u001b[0m synset_list \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49msynsets(string, pos)\n\u001b[1;32m     95\u001b[0m \u001b[39mfor\u001b[39;00m synset \u001b[39min\u001b[39;00m synset_list:\n\u001b[1;32m     96\u001b[0m     sentis\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msenti_synset(synset\u001b[39m.\u001b[39mname()))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/nlpenv/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1757\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synsets\u001b[0;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1756\u001b[0m         pos \u001b[39m=\u001b[39m POS_LIST\n\u001b[0;32m-> 1757\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m   1758\u001b[0m         get_synset(p, offset)\n\u001b[1;32m   1759\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m pos\n\u001b[1;32m   1760\u001b[0m         \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_morphy(lemma, p, check_exceptions)\n\u001b[1;32m   1761\u001b[0m         \u001b[39mfor\u001b[39;00m offset \u001b[39min\u001b[39;00m index[form]\u001b[39m.\u001b[39mget(p, [])\n\u001b[1;32m   1762\u001b[0m     ]\n\u001b[1;32m   1764\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1765\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_lang_data(lang)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/nlpenv/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1760\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1756\u001b[0m         pos \u001b[39m=\u001b[39m POS_LIST\n\u001b[1;32m   1757\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m   1758\u001b[0m         get_synset(p, offset)\n\u001b[1;32m   1759\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m pos\n\u001b[0;32m-> 1760\u001b[0m         \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_morphy(lemma, p, check_exceptions)\n\u001b[1;32m   1761\u001b[0m         \u001b[39mfor\u001b[39;00m offset \u001b[39min\u001b[39;00m index[form]\u001b[39m.\u001b[39mget(p, [])\n\u001b[1;32m   1762\u001b[0m     ]\n\u001b[1;32m   1764\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1765\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_lang_data(lang)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/nlpenv/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:2072\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_morphy\u001b[39m(\u001b[39mself\u001b[39m, form, pos, check_exceptions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   2065\u001b[0m     \u001b[39m# from jordanbg:\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m     \u001b[39m# Given an original string x\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[39m# 3. If there are no matches, keep applying rules until you either\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m     \u001b[39m#    find a match or you can't go any further\u001b[39;00m\n\u001b[0;32m-> 2072\u001b[0m     exceptions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exception_map[pos]\n\u001b[1;32m   2073\u001b[0m     substitutions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001b[1;32m   2075\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'P'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def get_sentiment(word, pos):\n",
    "    # Get the sentiment scores for a given word and part of speech\n",
    "    synsets = list(swn.senti_synsets(word, pos))\n",
    "    if synsets:\n",
    "        # Use the average sentiment scores of all the synsets\n",
    "        sentiment = sum(s.pos_score() - s.neg_score() for s in synsets) / len(synsets)\n",
    "        return sentiment\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_word_sentiments(sentence):\n",
    "    # Tokenize the sentence and get the part of speech for each word\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Map Penn Treebank POS tags to WordNet POS tags\n",
    "    pos_mapping = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    word_sentiments = []\n",
    "\n",
    "    for word, pos in pos_tags:\n",
    "        # Convert Penn Treebank POS tags to WordNet POS tags\n",
    "        word_pos = pos[0].upper()\n",
    "        if word_pos in pos_mapping:\n",
    "            word_pos = pos_mapping[word_pos]\n",
    "\n",
    "        # Get the sentiment score for the word\n",
    "        sentiment = get_sentiment(word, word_pos)\n",
    "        word_sentiments.append((word, sentiment))\n",
    "\n",
    "    return word_sentiments\n",
    "\n",
    "# Example usage\n",
    "sentence = \"I love this product. It's amazing!\"\n",
    "word_sentiments = get_word_sentiments(sentence)\n",
    "for word, sentiment in word_sentiments:\n",
    "    print(f\"{word}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 0.0\n",
      "hate: -0.75\n",
      "this: 0.0\n",
      "product: 0.0\n",
      ".: 0.0\n",
      "It: 0.0\n",
      "'s: 0.0\n",
      "amazing: 0.5\n",
      "!: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def get_sentiment(word, pos):\n",
    "    # Get the sentiment scores for a given word and part of speech\n",
    "    synsets = list(swn.senti_synsets(word, pos))\n",
    "    if synsets:\n",
    "        # Use the average sentiment scores of all the synsets\n",
    "        sentiment = sum(s.pos_score() - s.neg_score() for s in synsets) / len(synsets)\n",
    "        return sentiment\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_word_sentiments(sentence):\n",
    "    # Tokenize the sentence and get the part of speech for each word\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Map Penn Treebank POS tags to WordNet POS tags\n",
    "    pos_mapping = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    word_sentiments = []\n",
    "\n",
    "    for word, pos in pos_tags:\n",
    "        # Convert Penn Treebank POS tags to WordNet POS tags\n",
    "        word_pos = pos[0].upper()\n",
    "        word_pos = pos_mapping.get(word_pos, 'n')  # Use 'n' as the default value if not found\n",
    "\n",
    "        # Get the sentiment score for the word\n",
    "        sentiment = get_sentiment(word, word_pos)\n",
    "        word_sentiments.append((word, sentiment))\n",
    "\n",
    "    return word_sentiments\n",
    "\n",
    "# Example usage\n",
    "sentence = \"I hate this product. It's amazing!\"\n",
    "word_sentiments = get_word_sentiments(sentence)\n",
    "for word, sentiment in word_sentiments:\n",
    "    print(f\"{word}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
